"""
Script de comparaison des agents RL et ML.
Phase 4 du projet : Analyse comparative.
"""

import sys
import os
import argparse
import json

# Ajouter le chemin du projet
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config import (
    Q_TABLE_FILE, KNN_MODEL_FILE, RESULTS_DIR, EVAL_EPISODES
)
from src.agents.q_learning_agent import QLearningAgent
from src.agents.knn_agent import KNNAgent
from src.utils.metrics import AgentEvaluator, analyze_stability
from src.utils.visualization import plot_comparison_results


def compare_agents(
    rl_agent_path: str = None,
    ml_agent_path: str = None,
    num_episodes: int = EVAL_EPISODES,
    analyze_stab: bool = True,
    show_plots: bool = True,
    save_results: bool = True
):
    """
    Compare les performances des agents RL et ML.
    
    Args:
        rl_agent_path: Chemin de l'agent Q-learning
        ml_agent_path: Chemin de l'agent k-NN
        num_episodes: Nombre d'√©pisodes d'√©valuation
        analyze_stab: Analyser la stabilit√©
        show_plots: Afficher les graphiques
        save_results: Sauvegarder les r√©sultats
    """
    print("=" * 60)
    print("üî¨ PHASE 4 : COMPARAISON RL vs ML")
    print("=" * 60)
    
    if rl_agent_path is None:
        rl_agent_path = Q_TABLE_FILE
    
    if ml_agent_path is None:
        ml_agent_path = KNN_MODEL_FILE
    
    # Charger les agents
    print(f"\nüì• Chargement des agents...")
    
    rl_agent = QLearningAgent()
    ml_agent = KNNAgent()
    
    try:
        rl_agent.load(rl_agent_path)
    except FileNotFoundError:
        print(f"\n‚ùå Erreur: Agent RL non trouv√© √† {rl_agent_path}")
        print("   Veuillez d'abord ex√©cuter train_rl.py")
        return None
    
    try:
        ml_agent.load(ml_agent_path)
    except FileNotFoundError:
        print(f"\n‚ùå Erreur: Agent ML non trouv√© √† {ml_agent_path}")
        print("   Veuillez d'abord ex√©cuter train_ml.py")
        return None
    
    print(f"   ‚úÖ Agent RL charg√©")
    print(f"   ‚úÖ Agent ML charg√© (k={ml_agent.n_neighbors})")
    
    print(f"\nüìã Configuration:")
    print(f"   √âpisodes d'√©valuation: {num_episodes}")
    
    # √âvaluer et comparer
    evaluator = AgentEvaluator()
    comparison = evaluator.compare_agents(rl_agent, ml_agent, num_episodes)
    
    # Analyse de stabilit√©
    if analyze_stab:
        print("\n" + "=" * 60)
        print("üìà ANALYSE DE STABILIT√â")
        print("=" * 60)
        
        rl_stability = analyze_stability(rl_agent, 'rl', num_runs=5, episodes_per_run=50)
        ml_stability = analyze_stability(ml_agent, 'ml', num_runs=5, episodes_per_run=50)
        
        comparison['rl_stability'] = rl_stability
        comparison['ml_stability'] = ml_stability
        
        print("\nüìä Comparaison de stabilit√©:")
        print(f"   RL - CV score: {rl_stability['cv_score']:.2%}")
        print(f"   ML - CV score: {ml_stability['cv_score']:.2%}")
        
        if rl_stability['cv_score'] < ml_stability['cv_score']:
            print(f"   ‚Üí RL est plus stable (moins de variation)")
        else:
            print(f"   ‚Üí ML est plus stable (moins de variation)")
    
    # Conclusions
    print("\n" + "=" * 60)
    print("üìù CONCLUSIONS")
    print("=" * 60)
    
    rl_metrics = comparison['rl']
    ml_metrics = comparison['ml']
    
    print("\nüéØ Avantages du Reinforcement Learning (Q-Learning):")
    print("   ‚Ä¢ Apprend directement par interaction avec l'environnement")
    print("   ‚Ä¢ S'adapte naturellement aux changements de l'environnement")
    print("   ‚Ä¢ Ne n√©cessite pas de donn√©es √©tiquet√©es")
    print("   ‚Ä¢ Optimise directement la politique de d√©cision")
    
    print("\nüìä Avantages du Machine Learning Supervis√© (k-NN):")
    print("   ‚Ä¢ Entra√Ænement tr√®s rapide une fois le dataset disponible")
    print("   ‚Ä¢ Pr√©dictions rapides et simples")
    print("   ‚Ä¢ Facilement interpr√©table")
    print("   ‚Ä¢ Pas de phase d'exploration (stabilit√© imm√©diate)")
    
    print("\n‚ö†Ô∏è Limites:")
    print("   ‚Ä¢ RL: Temps d'entra√Ænement long, exploration peut √™tre dangereuse")
    print("   ‚Ä¢ ML: D√©pend de la qualit√© du dataset, ne s'adapte pas aux changements")
    
    print("\nüèÜ Recommandation:")
    if rl_metrics['avoidance_rate'] >= ml_metrics['avoidance_rate']:
        print("   Pour un environnement dynamique et √©volutif: RL (Q-Learning)")
    print("   Pour un d√©ploiement rapide avec environnement stable: ML (k-NN)")
    
    # Sauvegarder les r√©sultats
    if save_results:
        # Pr√©parer les r√©sultats pour JSON (enlever les listes numpy)
        results_to_save = {
            'num_episodes': num_episodes,
            'rl': {
                'avoidance_rate': rl_metrics['avoidance_rate'],
                'collision_rate': rl_metrics['collision_rate'],
                'avg_reward': rl_metrics['avg_reward'],
                'avg_episode_length': rl_metrics['avg_episode_length'],
                'total_score': rl_metrics['total_score']
            },
            'ml': {
                'avoidance_rate': ml_metrics['avoidance_rate'],
                'collision_rate': ml_metrics['collision_rate'],
                'avg_reward': ml_metrics['avg_reward'],
                'avg_episode_length': ml_metrics['avg_episode_length'],
                'total_score': ml_metrics['total_score']
            }
        }
        
        results_path = os.path.join(RESULTS_DIR, "comparison_results.json")
        os.makedirs(RESULTS_DIR, exist_ok=True)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ R√©sultats sauvegard√©s: {results_path}")
    
    # Graphiques
    if show_plots:
        print("\nüìà G√©n√©ration des graphiques...")
        
        plot_comparison_results(
            comparison,
            save_path=os.path.join(RESULTS_DIR, "comparison_results.png"),
            show=True
        )
    
    print("\n‚úÖ Comparaison termin√©e!")
    
    return comparison


def main():
    parser = argparse.ArgumentParser(
        description="Comparaison des agents RL et ML"
    )
    parser.add_argument(
        '--rl-agent',
        type=str,
        default=None,
        help="Chemin de l'agent Q-learning"
    )
    parser.add_argument(
        '--ml-agent',
        type=str,
        default=None,
        help="Chemin de l'agent k-NN"
    )
    parser.add_argument(
        '--episodes', '-e',
        type=int,
        default=EVAL_EPISODES,
        help=f"Nombre d'√©pisodes d'√©valuation (d√©faut: {EVAL_EPISODES})"
    )
    parser.add_argument(
        '--no-stability',
        action='store_true',
        help="Ne pas analyser la stabilit√©"
    )
    parser.add_argument(
        '--no-plots',
        action='store_true',
        help="Ne pas afficher les graphiques"
    )
    parser.add_argument(
        '--no-save',
        action='store_true',
        help="Ne pas sauvegarder les r√©sultats"
    )
    
    args = parser.parse_args()
    
    compare_agents(
        rl_agent_path=args.rl_agent,
        ml_agent_path=args.ml_agent,
        num_episodes=args.episodes,
        analyze_stab=not args.no_stability,
        show_plots=not args.no_plots,
        save_results=not args.no_save
    )


if __name__ == "__main__":
    main()
